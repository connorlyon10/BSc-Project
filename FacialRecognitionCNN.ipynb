{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This convolutional network (CN) recognises faces from a database of 244 images of 16 individuals. The images are low in quality and variability. The aim of using the chosen data was to see how CN's respond to low quality data, compared to easily classifiable high-quality  data such as the MNIST data set.\n",
        "\n",
        "The model is successful at matching a face to an individual ~80% of time. Given the limitations in the data set this is still quite impressive. The comparison in results here and in the MNIST model demonstrate how important data quality is."
      ],
      "metadata": {
        "id": "ZAjNVCmVbgdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "7Ic7yXo5xnkI",
        "outputId": "bdcabf24-86b5-45df-f897-bc7ba62564e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 244 images belonging to 16 classes.\n",
            "Found 244 images belonging to 16 classes.\n",
            "Mapping of Face and its ID {'face1': 0, 'face10': 1, 'face11': 2, 'face12': 3, 'face13': 4, 'face14': 5, 'face15': 6, 'face16': 7, 'face2': 8, 'face3': 9, 'face4': 10, 'face5': 11, 'face6': 12, 'face7': 13, 'face8': 14, 'face9': 15}\n",
            "\n",
            " The Number of output neurons:  16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m2,432\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m51,264\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10816\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │         \u001b[38;5;34m692,288\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m1,040\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10816</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">692,288</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m747,024\u001b[0m (2.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">747,024</span> (2.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m747,024\u001b[0m (2.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">747,024</span> (2.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 537ms/step - accuracy: 0.0654 - loss: 67.5777 - val_accuracy: 0.0902 - val_loss: 3.7819\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 483ms/step - accuracy: 0.1259 - loss: 3.0441 - val_accuracy: 0.1148 - val_loss: 2.6606\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 690ms/step - accuracy: 0.1728 - loss: 2.6270 - val_accuracy: 0.3689 - val_loss: 2.0852\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 457ms/step - accuracy: 0.3624 - loss: 2.0565 - val_accuracy: 0.7336 - val_loss: 1.1424\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 464ms/step - accuracy: 0.6533 - loss: 1.2802 - val_accuracy: 0.7828 - val_loss: 0.7175\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "###### Total Time Taken:  2 Minutes ######\n"
          ]
        }
      ],
      "source": [
        "# Deep Learning CNN model to recognize face\n",
        "# This script uses an existing database of images and creates a CNN to recognise faces in the images.\n",
        "# Data extracted from Farukh Hashmi's blog\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# IMAGE PRE-PROCESSING for TRAINING and TESTING data\n",
        "\n",
        "# Specifying the location of the training data\n",
        "from google.colab import drive\n",
        "import time\n",
        "drive.mount('/content/drive')\n",
        "TrainingImagePath = '/content/drive/My Drive/FaceImages/TrainingImages'\n",
        "\n",
        "\n",
        "# Defining pre-processing transformations on raw images of training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "        shear_range=0.15,\n",
        "        zoom_range=0.15,\n",
        "        horizontal_flip=True\n",
        "        )\n",
        "# Defining pre-processing transformations on raw images of validation data\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "\n",
        "# Generating the Training Data\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        ")\n",
        "# Generating the Validation Data\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Normalising the data.\n",
        "# training_set = tf.keras.utils.normalize(training_set, axis = 1)\n",
        "# test_set = tf.keras.utils.normalize(test_set, axis = 1)\n",
        "\n",
        "# Printing class labels for each face\n",
        "test_set.class_indices\n",
        "print(\"Mapping of Face and its ID\", test_set.class_indices)\n",
        "\n",
        "# The number of neurons in the output layer is the number of people in the data\n",
        "OutputNeurons = len(test_set.class_indices)\n",
        "print('\\n The Number of output neurons: ', OutputNeurons)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "# Creating the CNN architechture\n",
        "\n",
        "input_shape = (64, 64, 3)\n",
        "inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "##### Convolutional layers #####\n",
        "layer = tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), activation=tf.nn.relu)(inputs)\n",
        "layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "\n",
        "layer = tf.keras.layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), activation=tf.nn.relu)(layer)\n",
        "layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "##### --------------------- #####\n",
        "\n",
        "##### Flattening layer #####\n",
        "layer = tf.keras.layers.Flatten()(layer)\n",
        "##### ---------------- #####\n",
        "\n",
        "##### Fully connected layer #####\n",
        "layer = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(layer)\n",
        "##### ----------------------- #####\n",
        "\n",
        "##### Output layer #####\n",
        "outputs = tf.keras.layers.Dense(units=OutputNeurons, activation=tf.nn.softmax)(layer)\n",
        "##### ------------ #####\n",
        "\n",
        "\n",
        "# Compile the CNN\n",
        "model = tf.keras.Model(inputs, outputs)  # Initialize the CNN\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "StartTime = time.time()\n",
        "\n",
        "model.fit(\n",
        "    training_set,\n",
        "    steps_per_epoch=len(training_set),  # Adjusted based on the number of batches per epoch\n",
        "    epochs=10,\n",
        "    validation_data=test_set,\n",
        "    validation_steps=len(test_set)\n",
        ")\n",
        "EndTime = time.time()\n",
        "\n",
        "print(\"###### Total Time Taken: \", round((EndTime - StartTime) / 60), 'Minutes ######')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wF-IlSjwM2F"
      },
      "source": [
        "The code cell below runs the model *num_runs* times, to find an average accuracy. The inherently stochastic process of neural network modelling is much more noticable for smaller data sets - some runs yield a 90% accuracy, whereas some runs only yield a 65% accuracy. **WARNING:** will take **2 minutes × num_runs** to run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOUR9Oa8oLD3",
        "outputId": "83fcee5b-bcc1-4089-f269-1868bbd209f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning! Expect this code cell to take 10 minutes to execute!\n",
            "\n",
            "----- Run 1 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Run 1: 0.8237704634666443\n",
            "\n",
            "----- Run 2 -----\n",
            "Accuracy for Run 2: 0.7459016442298889\n",
            "\n",
            "----- Run 3 -----\n",
            "Accuracy for Run 3: 0.8483606576919556\n",
            "\n",
            "----- Run 4 -----\n",
            "Accuracy for Run 4: 0.8360655903816223\n",
            "\n",
            "----- Run 5 -----\n",
            "Accuracy for Run 5: 0.4836065471172333\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "num_runs = 5\n",
        "print(f\"Warning! Expect this code cell to take {num_runs * 2} minutes to execute!\")\n",
        "for i in range(1, num_runs + 1):\n",
        "    print(f\"\\n----- Run {i} -----\")\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), activation=tf.nn.relu,\n",
        "                               input_shape=(64, 64, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), activation=tf.nn.relu),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=64, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(units=OutputNeurons, activation=tf.nn.softmax)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        training_set,\n",
        "        steps_per_epoch=len(training_set),\n",
        "        epochs=10,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=len(test_set),\n",
        "        verbose=0  # Set verbose to 0 to suppress training output\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on test set\n",
        "    _, accuracy = model.evaluate(test_set, verbose=0)\n",
        "\n",
        "    print(f\"Accuracy for Run {i}: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vquNKBQX5J0t"
      },
      "source": [
        "Now the CNN is trained with the data set predictions can be made. The cell below extracts a single testing image and resizes it to the model requirements. The resized image is given as input to the model, which returns an output **result**.\n",
        "\n",
        "Some basic data extraction is done to find the ID of the real image and of the predicted image. Finally a plot of the input titled with it's real and predicted class is given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUw-3JcP9hJn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Define image paths for all images\n",
        "image_paths = [\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face10/1face10.jpg',\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face14/2face14.jpg',\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face6/1face6.jpg',\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face1/3face1.jpg',\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face2/1face2.jpg',\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face13/1face13.jpg'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 8.5))\n",
        "\n",
        "# Iterate over each image path\n",
        "for i, image_path in enumerate(image_paths, 1):\n",
        "    test_image = image.load_img(image_path, target_size=(64, 64))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    result = model.predict(test_image, verbose=0)  # Using the trained model for prediction\n",
        "\n",
        "    # Manual output based on the class indices\n",
        "    class_indices = {'face1': 0, 'face10': 1, 'face11': 2, 'face12': 3, 'face13': 4, 'face14': 5, 'face15': 6, 'face16': 7, 'face2': 8, 'face3': 9, 'face4': 10, 'face5': 11, 'face6': 12, 'face7': 13, 'face8': 14, 'face9': 15}\n",
        "\n",
        "    # Extracting the actual face number from the file name\n",
        "    file_name = image_path.split('/')[-1]  # Extracting the file name from the path\n",
        "    actual_face_number = int(file_name.split('face')[1].split('.')[0])  # Extracting the face number from the file name\n",
        "\n",
        "    # Manual output based on the manual class indices\n",
        "    predicted_class_index = np.argmax(result)  # Get the index of the predicted class\n",
        "    predicted_class = [k for k, v in class_indices.items() if v == predicted_class_index][0]  # Get the predicted class label\n",
        "\n",
        "    # Extracting only the numeric part from the predicted class\n",
        "    predicted_class_numeric = int(predicted_class.split('face')[1])  # Extracting the numeric part from the predicted class\n",
        "\n",
        "    # Plot the images\n",
        "    plt.subplot(2, 3, i)\n",
        "    plt.imshow(image.load_img(image_path))  # Plot the image\n",
        "    plt.title(f'Actual Face Number: {actual_face_number}\\nPredicted Class: {predicted_class_numeric}')  # Set the title with the actual face number and predicted class\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nCiFjoXt8GZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to the testing images folder\n",
        "testing_images_folder = '/content/drive/My Drive/FaceImages/TestingImages/'\n",
        "\n",
        "# Initialize an empty DataFrame to store the predictions\n",
        "predictions_df = pd.DataFrame(columns=['Actual Class'] + [f'Image {i}' for i in range(1, 5)])\n",
        "\n",
        "# Iterate over each class folder in the testing images folder\n",
        "for class_folder in os.listdir(testing_images_folder):\n",
        "    class_path = os.path.join(testing_images_folder, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        # Get the class label (e.g., 'face1', 'face2', etc.)\n",
        "        class_label = class_folder.split('face')[1]\n",
        "\n",
        "        # Initialize a list to store the predictions for each image in the class\n",
        "        class_predictions = [f'face{class_label}']\n",
        "\n",
        "        # Iterate over each image in the class folder\n",
        "        for i in range(1, 5):\n",
        "            image_path = os.path.join(class_path, f'{i}face{class_label}.jpg')\n",
        "            if os.path.isfile(image_path):\n",
        "                # Load and preprocess the image\n",
        "                test_image = image.load_img(image_path, target_size=(64, 64))\n",
        "                test_image = image.img_to_array(test_image)\n",
        "                test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "                # Predict the class label using the trained model\n",
        "                result = model.predict(test_image, verbose=0)\n",
        "                predicted_class_index = np.argmax(result)\n",
        "                predicted_class_label = [k for k, v in class_indices.items() if v == predicted_class_index][0]\n",
        "\n",
        "                # Compare the predicted class with the actual class\n",
        "                if predicted_class_label == class_folder:\n",
        "                    class_predictions.append(1)  # Correct prediction\n",
        "                else:\n",
        "                    class_predictions.append(0)  # Incorrect prediction\n",
        "            else:\n",
        "                # Image file does not exist\n",
        "                class_predictions.append(None)\n",
        "\n",
        "        # Append the class predictions to the DataFrame\n",
        "        predictions_df.loc[len(predictions_df)] = class_predictions\n",
        "\n",
        "# Display the predictions DataFrame\n",
        "print(predictions_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM9FXujpv8s7"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage accuracy\n",
        "accuracy = (predictions_df.iloc[:, 1:] == 1).mean().mean() * 100\n",
        "\n",
        "print(f\"Percentage Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SOEWMcuzpsi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Redirect stdout to suppress verbose output\n",
        "sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "# Define intermediate models to extract activations from specific layers\n",
        "conv1_output_model = Model(inputs=model.input, outputs=model.layers[2].output)\n",
        "\n",
        "# List of image paths\n",
        "image_paths = [\n",
        "    '/content/drive/My Drive/FaceImages/TestingImages/face10/1face10.jpg',\n",
        "    # Add more image paths as needed\n",
        "]\n",
        "\n",
        "#    '/content/drive/My Drive/FaceImages/TestingImages/face12/1face12.jpg',\n",
        "#    '/content/drive/My Drive/FaceImages/TestingImages/face13/1face13.jpg',\n",
        "\n",
        "# Iterate over each image path\n",
        "for idx, image_path in enumerate(image_paths, 1):\n",
        "    # Load the image and preprocess it\n",
        "    test_image = image.load_img(image_path, target_size=(64, 64))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    # Get activations from the first convolutional layer\n",
        "    conv1_output = conv1_output_model.predict(test_image)\n",
        "\n",
        "    # Plot the output of the first convolutional layer as a single image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(conv1_output[0, :, :, 13], cmap='viridis')  # Display the output of the first channel\n",
        "    #plt.title(f'Activations for Image {idx}')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "\n",
        "# Restore stdout\n",
        "sys.stdout = sys.__stdout__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4JyXxPTlivG"
      },
      "outputs": [],
      "source": [
        "activation_model = tf.keras.Model(inputs=model.input, outputs=model.layers[1].output)   #[1] represents the first conv layer. Try changing to [2] to see the output of the 2nd Conv2d layer.\n",
        "\n",
        "\n",
        "image_index = 1                                     #Change image_index for a different image\n",
        "image = x_test[image_index].reshape(1, 28, 28, 1)\n",
        "\n",
        "activations = activation_model.predict(image)\n",
        "\n",
        "#Plot the activations for the first image                          #This creates a lattice of the 64 outputs for the image.\n",
        "plt.figure(figsize=(8, 8))\n",
        "for i in range(64):\n",
        "    plt.subplot(8, 8, i+1)\n",
        "    plt.imshow(activations[0, :, :, i], cmap='viridis')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# plt.imshow(activations[0, :, :, 1], cmap='viridis')               #This shows just one image from the lattice above. Change 1 to any value 0<=x<63.\n",
        "# plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn5S4g98mj3m"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define intermediate models to extract activations from specific layers\n",
        "conv1_output_model = Model(inputs=model.input, outputs=model.layers[5].output)\n",
        "conv2_output_model = Model(inputs=model.input, outputs=model.layers[2].output)\n",
        "\n",
        "# Load the image and preprocess it\n",
        "test_image = image.load_img(image_path, target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Get activations from each convolutional layer\n",
        "conv1_output = conv1_output_model.predict(test_image)\n",
        "conv2_output = conv2_output_model.predict(test_image)\n",
        "\n",
        "# Plot the original image, output of the first convolutional layer, and output of the second convolutional layer\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image.load_img(image_path))\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Output of the first convolutional layer\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(conv1_output[0, :, :, 1])  # Plot the first channel of the output\n",
        "plt.title('The Convolutional Layer')\n",
        "plt.axis('off')\n",
        "\n",
        "# Output of the second convolutional layer\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(conv2_output[0, :, :, 1])  # Plot the first channel of the output\n",
        "plt.title('The Pooling Layer')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqVUb0rQ0vgSlukwu/8aw7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}